{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbc249-a54e-47a9-a7b4-d858379c7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c https://zenodo.org/records/3338373/files/musdb18hq.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274b4b6-dec8-4969-bbb6-c020284ec014",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip musdb18hq.zip -d musdb18hq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2fbbfd-2397-40d6-839e-3b212cd097ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from glob import glob\n",
    "from datasets import Dataset, Features, Audio, Value\n",
    "from IPython.display import Audio\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13efe4e-510c-4150-9a35-4e3c5dfc04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = (2**21)\n",
    "tracks = [path.split(\"/\")[2] for path in glob(\"musdb18hq/train/*/*.wav\")][::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c251d32-70ed-42ee-8887-9a20e9b800d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_track, track in enumerate(tracks):\n",
    "    # load the mixture and vocal tracks\n",
    "    mix_path = f\"musdb18hq/train/{track}/mixture.wav\"\n",
    "    vocal_path = f\"musdb18hq/train/{track}/vocals.wav\"\n",
    "    x,fs = torchaudio.load(mix_path, normalize=False)\n",
    "    v,fs = torchaudio.load(vocal_path, normalize=False)\n",
    "    C, L = v.shape\n",
    "    assert(x.shape == v.shape)\n",
    "    assert(fs==44100)\n",
    "    assert(C==2)\n",
    "    assert(x.dtype == v.dtype == torch.int16)\n",
    "    if L<=N//2:\n",
    "        continue\n",
    "\n",
    "    if (L%N)/N > 0.5:\n",
    "        # pad\n",
    "        B = L//N + 1\n",
    "        pad_length = B * N - L\n",
    "        x = torch.nn.functional.pad(x, (0, pad_length))\n",
    "        v = torch.nn.functional.pad(v, (0, pad_length))\n",
    "    else:\n",
    "        # drop last segment\n",
    "        B = L//N\n",
    "        x = x[:,:(B*N)]\n",
    "        v = v[:,:(B*N)]\n",
    "        \n",
    "    # Split the file into non-overlapping 48-second chunks\n",
    "    x = rearrange(x, 'C (B N) -> B C N', B=B, N=N)\n",
    "    v = rearrange(v, 'C (B N) -> B C N', B=B, N=N)\n",
    "    \n",
    "    # remove segments that don't have enough vocals\n",
    "    p = v.to(torch.float).norm(dim=1).mean(dim=1)\n",
    "    x = x[p>200]\n",
    "    v = v[p>200]\n",
    "    B = x.shape[0]\n",
    "    \n",
    "    # TODO: save each of the\n",
    "    for i_seg in range(B):\n",
    "        mix_file = f\"musdb_vss/{track}/vocals/{i_seg}.wav\"\n",
    "        vocal_file = f\"musdb_vss/{track}/mixture/{i_seg}.wav\"\n",
    "        mix_dir = os.path.dirname(mix_file)\n",
    "        vocal_dir = os.path.dirname(vocal_file)\n",
    "        os.makedirs(mix_dir, exist_ok=True)\n",
    "        os.makedirs(vocal_dir, exist_ok=True)\n",
    "        torchaudio.save(\n",
    "            uri = mix_file,\n",
    "            src = v[i_seg],\n",
    "            sample_rate = fs,\n",
    "        )\n",
    "        torchaudio.save(\n",
    "            uri = vocal_file,\n",
    "            src = x[i_seg],\n",
    "            sample_rate = fs,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

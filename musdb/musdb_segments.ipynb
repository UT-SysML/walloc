{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbc249-a54e-47a9-a7b4-d858379c7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c https://zenodo.org/records/3338373/files/musdb18hq.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274b4b6-dec8-4969-bbb6-c020284ec014",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip musdb18hq.zip -d musdb18hq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2fbbfd-2397-40d6-839e-3b212cd097ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from glob import glob\n",
    "from datasets import Dataset, Features, Audio, Value\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c251d32-70ed-42ee-8887-9a20e9b800d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=2**21\n",
    "for split in [\"train\", \"test\"]:\n",
    "    tracks = [path.split(\"/\")[2] for path in glob(f\"musdb18hq/{split}/*/*.wav\")][::5]\n",
    "    for i_track, track in enumerate(tracks):\n",
    "            mix_path = f\"musdb18hq/{split}/{track}/mixture.wav\"\n",
    "            vocal_path = f\"musdb18hq/{split}/{track}/vocals.wav\"\n",
    "            bass_path = f\"musdb18hq/{split}/{track}/bass.wav\"\n",
    "            drums_path = f\"musdb18hq/{split}/{track}/drums.wav\"\n",
    "            other_path = f\"musdb18hq/{split}/{track}/other.wav\"\n",
    "            x, fs = torchaudio.load(mix_path, normalize=False)\n",
    "            v, fs = torchaudio.load(vocal_path, normalize=False)\n",
    "            b, fs = torchaudio.load(bass_path, normalize=False)\n",
    "            d, fs = torchaudio.load(drums_path, normalize=False)\n",
    "            o, fs = torchaudio.load(other_path, normalize=False)\n",
    "            C, L = x.shape\n",
    "            assert(x.shape == v.shape == b.shape == d.shape == o.shape)\n",
    "            assert(fs==44100)\n",
    "            assert(C==2)\n",
    "            assert(x.dtype == v.dtype == b.dtype == d.dtype == o.dtype == torch.int16)\n",
    "        \n",
    "            if L<=N//2:\n",
    "                continue\n",
    "        \n",
    "            if (L%N)/N > 0.5:\n",
    "                # pad\n",
    "                B = L//N + 1\n",
    "                pad_length = B * N - L\n",
    "                x = torch.nn.functional.pad(x, (0, pad_length))\n",
    "                v = torch.nn.functional.pad(v, (0, pad_length))\n",
    "                b = torch.nn.functional.pad(b, (0, pad_length))\n",
    "                d = torch.nn.functional.pad(d, (0, pad_length))\n",
    "                o = torch.nn.functional.pad(o, (0, pad_length))\n",
    "            else:\n",
    "                # drop last segment\n",
    "                B = L//N\n",
    "                x = x[:, :(B * N)]\n",
    "                v = v[:, :(B * N)]\n",
    "                b = b[:, :(B * N)]\n",
    "                d = d[:, :(B * N)]\n",
    "                o = o[:, :(B * N)]\n",
    "                \n",
    "            # Split the file into non-overlapping 48-second chunks\n",
    "            x = rearrange(x, 'C (B N) -> B C N', B=B, N=N)\n",
    "            v = rearrange(v, 'C (B N) -> B C N', B=B, N=N)\n",
    "            b = rearrange(b, 'C (B N) -> B C N', B=B, N=N)\n",
    "            d = rearrange(d, 'C (B N) -> B C N', B=B, N=N)\n",
    "            o = rearrange(o, 'C (B N) -> B C N', B=B, N=N)\n",
    "            \n",
    "            # remove segments that don't have enough vocals\n",
    "            p = x.to(torch.float).norm(dim=1).mean(dim=1)\n",
    "            x = x[p > 200]\n",
    "            v = v[p > 200]\n",
    "            b = b[p > 200]\n",
    "            d = d[p > 200]\n",
    "            o = o[p > 200]\n",
    "            B = x.shape[0]\n",
    "            \n",
    "            # TODO: save each of the\n",
    "            for i_seg in range(B):\n",
    "                mix_file = f\"musdb_segments/{split}/{track}/mixture/{i_seg}.wav\"\n",
    "                vocal_file = f\"musdb_segments/{split}/{track}/vocals/{i_seg}.wav\"\n",
    "                bass_file = f\"musdb_segments/{split}/{track}/bass/{i_seg}.wav\"\n",
    "                drums_file = f\"musdb_segments/{split}/{track}/drums/{i_seg}.wav\"\n",
    "                other_file = f\"musdb_segments/{split}/{track}/other/{i_seg}.wav\"\n",
    "                mix_dir = os.path.dirname(mix_file)\n",
    "                vocal_dir = os.path.dirname(vocal_file)\n",
    "                bass_dir = os.path.dirname(bass_file)\n",
    "                drums_dir = os.path.dirname(drums_file)\n",
    "                other_dir = os.path.dirname(other_file)\n",
    "                os.makedirs(mix_dir, exist_ok=True)\n",
    "                os.makedirs(vocal_dir, exist_ok=True)\n",
    "                os.makedirs(bass_dir, exist_ok=True)\n",
    "                os.makedirs(drums_dir, exist_ok=True)\n",
    "                os.makedirs(other_dir, exist_ok=True)\n",
    "                torchaudio.save(\n",
    "                    uri=mix_file,\n",
    "                    src=x[i_seg],\n",
    "                    sample_rate=fs,\n",
    "                )\n",
    "                torchaudio.save(\n",
    "                    uri=vocal_file,\n",
    "                    src=v[i_seg],\n",
    "                    sample_rate=fs,\n",
    "                )\n",
    "                torchaudio.save(\n",
    "                    uri=bass_file,\n",
    "                    src=b[i_seg],\n",
    "                    sample_rate=fs,\n",
    "                )\n",
    "                torchaudio.save(\n",
    "                    uri=drums_file,\n",
    "                    src=d[i_seg],\n",
    "                    sample_rate=fs,\n",
    "                )\n",
    "                torchaudio.save(\n",
    "                    uri=other_file,\n",
    "                    src=o[i_seg],\n",
    "                    sample_rate=fs,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "278f37d2-a1d0-4acb-ba65-1129b74264a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "for split in [\"train\", \"test\"]:\n",
    "    tracks = list(set([p.split(\"/\")[2] for p in glob(f\"musdb_segments/{split}/*/*/*.wav\")]))\n",
    "    path_mix = []\n",
    "    path_vocal = []\n",
    "    path_bass = []\n",
    "    path_drums = []\n",
    "    path_other = []\n",
    "    audio_mix = []\n",
    "    audio_vocal = []\n",
    "    audio_bass = []\n",
    "    audio_drums = []\n",
    "    audio_other = []\n",
    "    for track in tracks:\n",
    "        segments = glob(f\"musdb_segments/{split}/{track}/mixture/*.wav\")\n",
    "        for i_seg in range(len(segments)):\n",
    "            mix_file = f\"musdb_segments/{split}/{track}/mixture/{i_seg}.wav\"\n",
    "            vocal_file = f\"musdb_segments/{split}/{track}/vocals/{i_seg}.wav\"\n",
    "            bass_file = f\"musdb_segments/{split}/{track}/bass/{i_seg}.wav\"\n",
    "            drums_file = f\"musdb_segments/{split}/{track}/drums/{i_seg}.wav\"\n",
    "            other_file = f\"musdb_segments/{split}/{track}/other/{i_seg}.wav\"\n",
    "            path_mix.append(mix_file)\n",
    "            path_vocal.append(vocal_file)\n",
    "            path_bass.append(bass_file)\n",
    "            path_drums.append(drums_file)\n",
    "            path_other.append(other_file)\n",
    "            audio_mix.append({'path': mix_file})\n",
    "            audio_vocal.append({'path': vocal_file})\n",
    "            audio_bass.append({'path': bass_file})\n",
    "            audio_drums.append({'path': drums_file})\n",
    "            audio_other.append({'path': other_file})\n",
    "\n",
    "    ds.append(Dataset.from_dict({\n",
    "        'audio_mix': audio_mix,\n",
    "        'audio_vocal': audio_vocal,\n",
    "        'audio_bass': audio_bass,\n",
    "        'audio_drums': audio_drums,\n",
    "        'audio_other': audio_other,\n",
    "        'path_mix': path_mix,\n",
    "        'path_vocal': path_vocal,\n",
    "        'path_bass': path_bass,\n",
    "        'path_drums': path_drums,\n",
    "        'path_other': path_other,\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee3e2c3-b7f3-4829-bbd7-98d400c68614",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features({\n",
    "    'audio_mix': Audio(sampling_rate=44100, mono=False, decode=False),\n",
    "    'audio_vocal': Audio(sampling_rate=44100, mono=False, decode=False),\n",
    "    'audio_bass': Audio(sampling_rate=44100, mono=False, decode=False),\n",
    "    'audio_drums': Audio(sampling_rate=44100, mono=False, decode=False),\n",
    "    'audio_other': Audio(sampling_rate=44100, mono=False, decode=False),\n",
    "    'path_mix': Value('string'),\n",
    "    'path_vocal': Value('string'),\n",
    "    'path_bass': Value('string'),\n",
    "    'path_drums': Value('string'),\n",
    "    'path_other': Value('string'),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba2a0ed-2352-455a-b752-e787c4c6d448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493d0057cb8949f8997937b626523eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = ds[0].cast(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daefabc3-e5a0-4948-968f-9d6e0daa48f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio_mix', 'audio_vocal', 'audio_bass', 'audio_drums', 'audio_other', 'path_mix', 'path_vocal', 'path_bass', 'path_drums', 'path_other'],\n",
       "    num_rows: 477\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1a93ac0-d561-41c6-951c-725576c197ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bd14dcc5f04ccabcfe40b6aa80e00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_ds = ds[1].cast(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a6610b2-c5d2-419a-87dc-c24827ae872c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio_mix', 'audio_vocal', 'audio_bass', 'audio_drums', 'audio_other', 'path_mix', 'path_vocal', 'path_bass', 'path_drums', 'path_other'],\n",
       "    num_rows: 262\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8da577-8fc3-48ab-860a-19fb189edfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e83e73fbe7c4519899c1fffe0c70cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5362003edd394e36880cf377c22360ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd87be85853d4d70a66afd34932bd707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds.push_to_hub(\"danjacobellis/musdb_segments\", split='train')\n",
    "valid_ds.push_to_hub(\"danjacobellis/musdb_segments\", split='validation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
